{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.4\n",
      "2.4.1+cu124\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofEpochs = 100\n",
    "batchSize = 32\n",
    "learningRateGenerator = 0.0002\n",
    "learningRateDiscriminator = 0.0002\n",
    "imageSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset = dsets.CIFAR10(root='./data', download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size = batchSize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False), \n",
    "            nn.BatchNorm2d(512), \n",
    "            nn.ReLU(True), \n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False), \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(128), \n",
    "            nn.ReLU(True), \n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False), \n",
    "            nn.BatchNorm2d(64), \n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias = False), \n",
    "            nn.LeakyReLU(0.2, inplace = True), \n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias = False), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace = True), \n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias = False), \n",
    "            nn.BatchNorm2d(256), \n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias = False), \n",
    "            nn.BatchNorm2d(512), \n",
    "            nn.LeakyReLU(0.2, inplace = True), \n",
    "\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input).view(-1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator().to(device)\n",
    "generator = Generator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizerDiscriminator = optim.Adam(Discriminator().parameters(), lr = learningRateDiscriminator, betas = (0.5, 0.999))\n",
    "optimizerGenerator = optim.Adam(Generator().parameters(), lr = learningRateGenerator, betas = (0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetGradientNorms(model):\n",
    "    totalNorm = 0\n",
    "    for p in model.parameters():\n",
    "        paramNorm = p.grad.data.norm(2)\n",
    "        totalNorm += paramNorm.item() ** 2\n",
    "    return totalNorm ** (1 / 2)\n",
    "\n",
    "generatorLosses = []\n",
    "discriminatorLosses = []\n",
    "generatorGradientNorms = []\n",
    "discriminatorGradientNorms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Finally, A study Folder\\\\Thapar Summer School on Machine Learning and Deep Learning\\\\GAN\\\\DC-GAN\\\\FakeImages'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveDir = os.path.join(os.getcwd(), \"FakeImages\")\n",
    "\n",
    "if not os.path.exists(saveDir):\n",
    "    os.makedirs(saveDir)\n",
    "\n",
    "saveDir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 0, Generator Loss: 0.6894029974937439, Discriminator Loss: 1.5193023681640625\n",
      "Epochs: 1, Generator Loss: 0.6518171429634094, Discriminator Loss: 1.5039211511611938\n",
      "Epochs: 2, Generator Loss: 0.652620792388916, Discriminator Loss: 1.4483940601348877\n",
      "Epochs: 3, Generator Loss: 0.6577266454696655, Discriminator Loss: 1.4484572410583496\n",
      "Epochs: 4, Generator Loss: 0.6859258413314819, Discriminator Loss: 1.4423766136169434\n",
      "Epochs: 5, Generator Loss: 0.6812514662742615, Discriminator Loss: 1.5000290870666504\n",
      "Epochs: 6, Generator Loss: 0.6983600854873657, Discriminator Loss: 1.528090000152588\n",
      "Epochs: 7, Generator Loss: 0.6836071014404297, Discriminator Loss: 1.49661386013031\n",
      "Epochs: 8, Generator Loss: 0.6891034245491028, Discriminator Loss: 1.5012075901031494\n",
      "Epochs: 9, Generator Loss: 0.6575510501861572, Discriminator Loss: 1.4722204208374023\n",
      "Epochs: 10, Generator Loss: 0.6856337785720825, Discriminator Loss: 1.4156017303466797\n",
      "Epochs: 11, Generator Loss: 0.6526297330856323, Discriminator Loss: 1.4460210800170898\n",
      "Epochs: 12, Generator Loss: 0.6645055413246155, Discriminator Loss: 1.423730731010437\n",
      "Epochs: 13, Generator Loss: 0.6597345471382141, Discriminator Loss: 1.5698003768920898\n",
      "Epochs: 14, Generator Loss: 0.6484309434890747, Discriminator Loss: 1.4631710052490234\n",
      "Epochs: 15, Generator Loss: 0.6492941379547119, Discriminator Loss: 1.5283076763153076\n",
      "Epochs: 16, Generator Loss: 0.670893669128418, Discriminator Loss: 1.4544343948364258\n",
      "Epochs: 17, Generator Loss: 0.6557648181915283, Discriminator Loss: 1.5094375610351562\n",
      "Epochs: 18, Generator Loss: 0.6858176589012146, Discriminator Loss: 1.4736344814300537\n",
      "Epochs: 19, Generator Loss: 0.6605252623558044, Discriminator Loss: 1.4336744546890259\n",
      "Epochs: 20, Generator Loss: 0.6846117973327637, Discriminator Loss: 1.4689918756484985\n",
      "Epochs: 21, Generator Loss: 0.6899833679199219, Discriminator Loss: 1.460389494895935\n",
      "Epochs: 22, Generator Loss: 0.6746350526809692, Discriminator Loss: 1.4810811281204224\n"
     ]
    }
   ],
   "source": [
    "fixedNoise = torch.randn(batchSize, 100, 1, 1, device = device)\n",
    "\n",
    "for epochs in range(numberofEpochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        realImages, _ = data\n",
    "        realImages = realImages.to(device)\n",
    "\n",
    "        optimizerDiscriminator.zero_grad()\n",
    "        \n",
    "        output = discriminator(realImages).view(-1)\n",
    "        realLabels = torch.ones(output.size()).to(device)\n",
    "        realLoss = criterion(output, realLabels)\n",
    "        realLoss.backward()\n",
    "\n",
    "        noise = torch.randn(128, 100, 1, 1, device = device)\n",
    "        fakeImages = generator(noise)\n",
    "        output = discriminator(fakeImages.detach()).view(-1)\n",
    "        fakeLabels = torch.zeros(output.size()).to(device)\n",
    "        fakeLoss = criterion(output, fakeLabels)\n",
    "        fakeLoss.backward()\n",
    "        discriminatorGradientNorms.append(GetGradientNorms(discriminator))\n",
    "        discriminatorGradientNorm = GetGradientNorms(discriminator)\n",
    "        optimizerDiscriminator.step()\n",
    "\n",
    "        optimizerGenerator.zero_grad()\n",
    "        output = discriminator(fakeImages).view(-1)\n",
    "        realLabels = torch.ones(output.size()).to(device)\n",
    "        generatorLoss = criterion(output, realLabels)\n",
    "        generatorLoss.backward()\n",
    "        generatorGradientNorms.append(GetGradientNorms(generator))\n",
    "        generatorGradientNorm = GetGradientNorms(generator)\n",
    "        optimizerGenerator.step()\n",
    "\n",
    "        generatorLosses.append(generatorLoss.item())\n",
    "        discriminatorLosses.append(realLoss.item() + fakeLoss.item())\n",
    "        generatorGradientNorms.append(generatorGradientNorm)\n",
    "        discriminatorGradientNorms.append(discriminatorGradientNorm)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake = generator(fixedNoise).detach().cpu()\n",
    "    vutils.save_image(fake, f\"{saveDir}/fake_{epochs+1}.png\", normalize = True)\n",
    "    print(f\"Epochs: {epochs}, Generator Loss: {generatorLoss}, Discriminator Loss: {realLoss + fakeLoss}\")\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(generatorLosses, label='Generator')\n",
    "    plt.plot(discriminatorLosses, label='Discriminator')\n",
    "    plt.legend()\n",
    "    plt.title(f\"Epochs: {epochs}, G Loss: {generatorLosses[-1]:.4f}, D Loss: {discriminatorLosses[-1]:.4f}\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(generatorGradientNorms, label='Generator')\n",
    "    plt.plot(discriminatorGradientNorms, label='Discriminator')\n",
    "    plt.legend()\n",
    "    plt.title(\"Gradient Norms\")\n",
    "    plt.savefig(f\"{saveDir}/epoch_{epochs}.png\")\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
